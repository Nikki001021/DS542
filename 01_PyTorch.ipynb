{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introducing Pytorch, CUDA and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note that PyTorch tensors and their associated operations are very similar to numpy n-dimensional arrays. A tensor is actually an n-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch build its library around Object Oriented Programming(OOP) concept. With object oriented programming, we orient our program design and structure around `objects`.\n",
    "\n",
    "The tensor in Pytorch is presented by the object `torch.tensor` --> which can be created from numpy ndarray objects.\n",
    "\n",
    "The two objects share memory --> making the transition between PyTorch and Numpy very cheap from a performance perspective.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "# Create a numpy array\n",
    "a = np.array([1, 2, 3])\n",
    "# Convert numpy array to torch tensor\n",
    "b = torch.from_numpy(a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Package                | Description                                                                                                           |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------------------------------|\n",
    "| torch                  | The top-level PyTorch package and tensor library.                                                                     |\n",
    "| torch.nn               | A subpackage that contains modules and extensible classes for building neural networks.                               |\n",
    "| torch.autograd         | A subpackage that supports all the differentiable Tensor operations in PyTorch.                                       |\n",
    "| torch.nn.functional    | A functional interface that contains operations used for building neural net like loss, activation, layer operations. |\n",
    "| torch.optim            | A subpackage that contains standard optimization operations like SGD and Adam.                                        |\n",
    "| torch.utils            | A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier.       |\n",
    "| torchvision            | A package that provides access to popular datasets, models, and image transformations for computer vision.            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use PyTorch for deep learning?\n",
    "\n",
    "- Pythonic\n",
    "- It’s written mostly in Python, and only drops into C++ and CUDA code for operations that are performance bottlenecks.\n",
    "- **It's Dynamic Computational Graph** (More Next Week!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On SCC - type `module avail torch` to see available versions of PyTorch\n",
    "- On SCC - type `module load torch` to load the latest version of PyTorch\n",
    "\n",
    "Check https://pytorch.org/ for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why deep learning uses GPUs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A GPU is a processor that is good at handling specialized computations.\n",
    "\n",
    "- In contrast to a central processing unit (CPU), which is a processor that is good at handling general computations.\n",
    "\n",
    "- A GPU can be much faster at computing than a CPU. However, this is not always the case.\n",
    "\n",
    "- The speed of a GPU relative to a CPU depends on the type of computation being performed. \n",
    "\n",
    "- The type of computation most suitable for a GPU is a computation that can be done in parallel.\n",
    "\n",
    "#### Parallel Computing:\n",
    "\n",
    "- Parallel computing is a type of computation where a particular computation is broken into independent smaller computations that can be carried out simultaneously.\n",
    "\n",
    "- The resulting computations are then recombined, or synchronized, to form the result of the original larger computation.\n",
    "The number of tasks that a larger task can be broken into depends on the `number of cores` contained on a particular piece of hardware.\n",
    "\n",
    "- Cores are the units that actually do the computation within a given processor, and CPUs typically have four, eight, or sixteen cores while GPUs have potentially thousands.\n",
    "\n",
    "#### So why deep learning uses them? - Neural networks are embarrassingly parallel.\n",
    "\n",
    "- Many of the computations that we do with neural networks can be easily broken into smaller computations in such a way that the set of smaller computations do not depend on one another. (Example: Convolution, more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU computing stack:\n",
    "\n",
    "- GPU as the hardware on the bottom\n",
    "\n",
    "- CUDA as the software architecture on top of the GPU\n",
    "\n",
    "- And finally libraries like cuDNN on top of CUDA. cuDNN - (CUDA Deep Neural Network library)\n",
    "\n",
    "- Sitting on top of CUDA and cuDNN is PyTorch\n",
    "\n",
    "All we need is to have a supported Nvidia GPU, and we can leverage CUDA using PyTorch. We don’t need to know how to use the CUDA API directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GPU computing stack](https://phucnsp.github.io/blog/images/copied_from_nb/data/dl_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following code:\n",
    "```python\n",
    "t = torch.tensor([1,2,3])\n",
    "```\n",
    "The tensor object created in this way is on the CPU by default. As a result, any operations that we do using this tensor object will be carried out on the CPU. Now, to move the tensor onto the GPU, we just write:\n",
    "```python\n",
    "t = t.cuda()\n",
    "```\n",
    "This ability makes PyTorch very flexible because computations can be selectively carried out either on the CPU or on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Note\n",
    "\n",
    "- GPU Can Be Slower Than CPU\n",
    "\n",
    "- GPU is only faster for particular (specialized) tasks\n",
    "\n",
    "- For example, moving data from the CPU to the GPU is costly, so in this case, the overall performance might be slower if the computation task is a simple one. Moving relatively small computational tasks to the GPU won’t speed us up very much and may indeed slow us down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Introducing Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors - Data Structures of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Tensor Operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://github.com/DL4DS/sp2024_notebooks/blob/main/release/disc01/00_fundamentals.ipynb -- Recommend Going through this notebook for an overview of PyTorch tensors and tensor operations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
